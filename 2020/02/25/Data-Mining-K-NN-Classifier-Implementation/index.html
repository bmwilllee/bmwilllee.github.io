<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Data Mining | K-nearest Neighbour(K-NN) Classifier Implementation | You Are Finally Here</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Author: Li Dongheng  _The Manhattan Distance was used to decide the distance between the data record, and originally training set was separated into training set and validation set, which take account">
<meta property="og:type" content="article">
<meta property="og:title" content="Data Mining | K-nearest Neighbour(K-NN) Classifier Implementation">
<meta property="og:url" content="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/index.html">
<meta property="og:site_name" content="You Are Finally Here">
<meta property="og:description" content="Author: Li Dongheng  _The Manhattan Distance was used to decide the distance between the data record, and originally training set was separated into training set and validation set, which take account">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/1.png">
<meta property="og:image" content="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/2.png">
<meta property="og:image" content="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/3.png">
<meta property="article:published_time" content="2020-02-24T22:58:38.000Z">
<meta property="article:modified_time" content="2020-02-25T00:53:11.047Z">
<meta property="article:author" content="Dongheng Li (Will)">
<meta property="article:tag" content="Data Mining">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/1.png">
  
    <link rel="alternate" href="/atom.xml" title="You Are Finally Here" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">You Are Finally Here</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Will&#39;s secret place</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://bmwilllee.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Data-Mining-K-NN-Classifier-Implementation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/" class="article-date">
  <time datetime="2020-02-24T22:58:38.000Z" itemprop="datePublished">2020-02-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/case/">case</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Data Mining | K-nearest Neighbour(K-NN) Classifier Implementation
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Author: Li Dongheng</p>
<hr>
<p>_The Manhattan Distance was used to decide the distance between the data record, and originally training set was separated into training set and validation set, which take account 80% and 20% of separately. After validation by testing different k, the highest accuracy produced by the classifier was 90.44% when k equals to 3 or 4.</p>
<hr>
<a id="more"></a>
<h2 id="Introduction-to-K-NN"><a href="#Introduction-to-K-NN" class="headerlink" title="Introduction to K-NN"></a>Introduction to K-NN</h2><p>The k-nearest neighbours algorithm (k-NN) is a non-parametric method used for classification and regression [1]. This algorithm first stores all training samples, then analyses (including election, calculation of weighted sum, etc.) the K nearest neighbours around a new sample, and then marks the new sample as the class with the highest frequency in the k-nearest neighbour point. This approach is sometimes called “instance-based learning” or “lazy learning”, in which we search for the nearest known eigenvector for a given input in order to predict it.</p>
<h3 id="Characteristics-of-KNN"><a href="#Characteristics-of-KNN" class="headerlink" title="Characteristics of KNN"></a>Characteristics of KNN</h3><ul>
<li>Simple and effective.</li>
<li>The need of storing all training sets, occupying a large amount of memory, the speed is relatively slow.</li>
<li>Before implementation, training sets are usually clustered to reduce data size.</li>
</ul>
<hr>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>In this case, I used K-NN firstly to train classifier and evaluate the accuracy with validation data. If the accuracy reaches more than 80% or higher, it indicates that K-NN is appropriate to deal with this problem. Instead, use other algorithms.</p>
<p>For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality [2]. Our training set has 9 dimensions, so there is a trade-off about whether conduct dimension reduction, this will depend on the error rate produced by validating phase.</p>
<h3 id="Distance-algorithm-for-K-NN"><a href="#Distance-algorithm-for-K-NN" class="headerlink" title="Distance algorithm for K-NN"></a>Distance algorithm for K-NN</h3><p>K-NN requires a reasonable algorithm for computing distance to decide the nearest neighbours. Since the value of our parameter only contains the Numbers 0, 1 and -1, Taxicab Distance is appropriate.</p>
<p><strong>Taxicab distance</strong> is also known as <strong>Manhattan distance</strong> or city block distance. The taxicab distance, $d$, between two vectors $p$, $q$ in an n-dimensional real vector space with fixed Cartesian coordinate system, is the sum of the lengths of the projections of the line segment between the points onto the coordinate axes. More formally,</p>
<script type="math/tex; mode=display">
d(p, q)=\sum\limits_{i=1}^{n}\mid p_{i}-q_{i}\mid</script><h3 id="Training-Data-validation-data-and-Testing-data"><a href="#Training-Data-validation-data-and-Testing-data" class="headerlink" title="Training Data, validation data and Testing data"></a>Training Data, validation data and Testing data</h3><p>The original training set includes 1253 data records, in order to evaluate the accuracy of classifier, 80% of the data is extracted as training data and rest 20% as validating data.<br><strong><em>Training phase:</em></strong></p>
<ul>
<li>Training set: 1002 columns</li>
<li>Validating set: 251 columns<br><strong><em>Testing phase:</em></strong></li>
<li>Training set: 1253 columns<br>Testing set: 100 columns</li>
</ul>
<h3 id="Modules-and-Tools"><a href="#Modules-and-Tools" class="headerlink" title="Modules and Tools"></a>Modules and Tools</h3><p>Since Python has rich modules for data processing, I used Python to implement K-NN. Here are modules I used.</p>
<ul>
<li><strong>Numpy</strong>: A basic module that provides efficient data processing and array support. Many modules rely on it, such as pandas.</li>
<li><strong>Pandas</strong>: Conduct data collection and analysis.</li>
<li><strong>Matplotlib</strong>: The underlying package for data visualisation.</li>
</ul>
<h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><ol>
<li>Entering training data. Pandas module helps entering data from excel file and store data into memory in a Data Frame format.</li>
<li>Separating data into 80% as training set and 20% as validating set.</li>
<li>Calculating distance between a single target record with all training set by using Manhattan Distance, then store distance by inserting into new column of Data Frame.</li>
<li>Sorting data by calculated distance attribute in an ascending order (that is, the shortest distance is arranged in front).</li>
<li>Getting the first k data records, these records represent the k-nearest neighbour.</li>
<li>Counting the class with the most frequency appears in k-nearest neighbour.</li>
<li>Output the class with most frequency as predicted result and store result in an array.</li>
<li>Applying the procedures above (3 to 7) to rest of validating set.</li>
<li>Calculating accuracy by counting total number of matches between predictions and given classes of validating set (Figure 1).</li>
<li>Apply classifier to testing set, and store prediction result.<br><img src="/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/1.png" alt="Figure 1: Results as k=3"></li>
</ol>
<hr>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><h3 id="Manhattan-Distance"><a href="#Manhattan-Distance" class="headerlink" title="Manhattan Distance"></a>Manhattan Distance</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Manhattan_Distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Manhattan</span><span class="params">(vec1, vec2)</span>:</span></span><br><span class="line">    npvec1, npvec2 = np.array(vec1), np.array(vec2)</span><br><span class="line">    <span class="keyword">return</span> np.abs(npvec1-npvec2).sum()</span><br></pre></td></tr></table></figure>
<h3 id="K-NN-Implementation-for-a-single-row-of-testing-record"><a href="#K-NN-Implementation-for-a-single-row-of-testing-record" class="headerlink" title="K-NN Implementation for a single row of testing record."></a>K-NN Implementation for a single row of testing record.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KnnOneRecord</span><span class="params">(vec, k, data, lineNum)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># vec: single test record</span></span><br><span class="line">    <span class="comment"># k: k-index (KNN)</span></span><br><span class="line">    <span class="comment"># data: all-data</span></span><br><span class="line">    <span class="comment"># LineNum: number of training data</span></span><br><span class="line">    <span class="comment"># InPut: single item in vector</span></span><br><span class="line">    <span class="comment"># OutPut: class</span></span><br><span class="line">    </span><br><span class="line">    trainData = data.loc[:lineNum] <span class="comment"># extract training data</span></span><br><span class="line">    dists = np.zeros(trainData.shape[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> trainData.index:</span><br><span class="line">        dist = Manhattan(vec, trainData.loc[line].values[:<span class="number">-1</span>])</span><br><span class="line">        dists[line] = dist</span><br><span class="line"></span><br><span class="line">    trainData.insert(<span class="number">10</span>,<span class="string">"dist"</span>, dists) <span class="comment"># insert distance column</span></span><br><span class="line">    indices = trainData.sort_values(by=[<span class="string">"dist"</span>]) <span class="comment"># sort data with DES</span></span><br><span class="line">    target = indices[:k] <span class="comment"># get k-nearest records</span></span><br><span class="line">    </span><br><span class="line">    classCount = &#123;&#125; <span class="comment">#Dic</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> target.index:</span><br><span class="line">        label = target.loc[line].values[<span class="number">-2</span>]</span><br><span class="line">        classCount[label] = classCount.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    sortedCount = dict(sorted(classCount.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> max(sortedCount, key=sortedCount.get) <span class="comment"># return the most one</span></span><br></pre></td></tr></table></figure>
<h3 id="The-code-for-validating-shows-as-below"><a href="#The-code-for-validating-shows-as-below" class="headerlink" title="The code for validating shows as below."></a>The code for validating shows as below.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KnnValidation</span><span class="params">(k, data, per)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># k: k-index (K-NN)</span></span><br><span class="line">    <span class="comment"># data: all-data</span></span><br><span class="line">    <span class="comment"># per: persentage of training data</span></span><br><span class="line">    <span class="comment"># InPut: All Data (Training data + valitation data)</span></span><br><span class="line">    <span class="comment"># OutPut: Accuracy Rate</span></span><br><span class="line">    </span><br><span class="line">    lineNum = int(len(data)*per)</span><br><span class="line">    valData = data.loc[lineNum:]</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    predicCol = np.zeros(valData.shape[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> valData.index: <span class="comment"># calculate prediction value for each record</span></span><br><span class="line">        prediction = KnnOneRecord(valData.loc[line].values[:<span class="number">-1</span>], k, data, lineNum<span class="number">-1</span>)</span><br><span class="line">        predicCol[num] = prediction</span><br><span class="line">        num+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span>(valData.loc[line].values[<span class="number">-1</span>] == prediction):</span><br><span class="line">            count+=<span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    valData.insert(<span class="number">10</span>,<span class="string">"prediction"</span>, predicCol)</span><br><span class="line">    accRate = count / len(valData)</span><br><span class="line">    print(valData)</span><br><span class="line">    <span class="keyword">return</span> accRate</span><br></pre></td></tr></table></figure>
<h3 id="Following-function-is-used-to-test-the-testing-set"><a href="#Following-function-is-used-to-test-the-testing-set" class="headerlink" title="Following function is used to test the testing set."></a>Following function is used to test the testing set.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KnnTesting</span><span class="params">(k, trainSet, TestSet)</span>:</span></span><br><span class="line">    <span class="comment"># k: k-index (K-NN)</span></span><br><span class="line">    <span class="comment"># trainSet</span></span><br><span class="line">    <span class="comment"># TestSet: persentage of training data</span></span><br><span class="line">    <span class="comment"># InPut: Training data + Testing data, k</span></span><br><span class="line">    <span class="comment"># OutPut: Output prediction into file</span></span><br><span class="line">    lineNum = len(trainSet)</span><br><span class="line">    predicCol = np.zeros(TestSet.shape[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> TestSet.index: <span class="comment"># calculate prediction value for each record</span></span><br><span class="line">        prediction = KnnOneRecord(TestSet.loc[line].values[:<span class="number">-1</span>], k, trainSet, lineNum)</span><br><span class="line">        predicCol[line] = prediction</span><br><span class="line"></span><br><span class="line">    TestSet.insert(<span class="number">10</span>,<span class="string">"prediction"</span>, predicCol)</span><br><span class="line">    print(TestSet)</span><br><span class="line">    TestSet.to_excel(<span class="string">'result.xlsx'</span>)</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img src="/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/2.png" alt="Figure 2: Predicted result"><br>As it is shown on the Figure 2, the prediction column has illustrated the predicted results for each record, add export to an excel file.</p>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><h3 id="Cost-Matrix"><a href="#Cost-Matrix" class="headerlink" title="Cost Matrix"></a>Cost Matrix</h3><p>A cost matrix (error matrix) is useful when specific classification errors. The correlate cost matrix (Table 1) shows as below:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th><strong>PREDICTED CLASS</strong></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ACTUAL</strong>  <strong>CLASS</strong></td>
<td>C(i\</td>
<td>j)</td>
<td>Class=1</td>
<td>Class=0</td>
<td>Class=-1</td>
</tr>
<tr>
<td>Class=1</td>
<td>C(1\</td>
<td>1)</td>
<td>C(0\</td>
<td>1)</td>
<td>C(-1\</td>
<td>1)</td>
<td>C(-1\</td>
<td>1)</td>
</tr>
<tr>
<td>Class=0</td>
<td>C(1\</td>
<td>0)</td>
<td>C(0\</td>
<td>0)</td>
<td>C(-1\</td>
<td>0)</td>
<td>C(-1\</td>
<td>0)</td>
</tr>
<tr>
<td>Class=-1</td>
<td>C(1\</td>
<td>-1)</td>
<td>C(0\</td>
<td>-1)</td>
<td>C(-1\</td>
<td>-1)</td>
<td>C(-1\</td>
<td>-1)</td>
</tr>
</tbody>
</table>
</div>
<p>In our case, the accuracy can be represented as,</p>
<script type="math/tex; mode=display">
Accuracy = \frac{C(1|0)+C(0|0)+C(-1|-1)}{N}</script><h3 id="Choosing-the-value-of-k"><a href="#Choosing-the-value-of-k" class="headerlink" title="Choosing the value of k"></a>Choosing the value of k</h3><p>In order to obtain the K-NN classifier with highest accuracy, different K should be test, and we would final choose K which has the highest accuracy by using formula above. Usually, K is small, here I chose K from 1 to 8 (Figure 3).</p>
<p><img src="/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/3.png" alt="Figure 3: Accuracy changing"></p>
<p>The highest accuracy appears when k = 3 or k = 4, which is 90.44%. Notably, there is a gradual raise for accuracy when k = 7, this may due to some outliers that effect the classification performance.<br>Overall, the average accuracy is higher than 88% if we choose k from 1 to 8, which means K-NN can have a good performance when handle simple classification task.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]N. S. Altman. (1992). An introduction to kernel and nearest-neighbor nonparametric regression”. The American Statistician. Volume 46, 1992 – Issue 3.<a href="https://en.wikipedia.org/wiki/Digital_object_identifier%22%20%5Co%20%22Digital%20object%20identifier" target="_blank" rel="noopener">doi:10.1080/00031305.1992.10475879</a><br>[2]Beyer &amp; Kevin. (1999). ‘When is “nearest neighbor” meaningful?’ Database Theory—ICDT 99, 217-235.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://bmwilllee.github.io/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/" data-id="ck716i58x0000dfm1e6jw701h" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/02/22/An-Automatic-Drink-Dispenser-Based-on-Arduino/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">An Automatic Drink Dispenser Based on Arduino</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/case/">case</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/25/Data-Mining-K-NN-Classifier-Implementation/">Data Mining | K-nearest Neighbour(K-NN) Classifier Implementation</a>
          </li>
        
          <li>
            <a href="/2020/02/22/An-Automatic-Drink-Dispenser-Based-on-Arduino/">An Automatic Drink Dispenser Based on Arduino</a>
          </li>
        
          <li>
            <a href="/2020/02/08/Volkswagen%20Emissions%20Scandal%20(Dieselgate,%20Emissionsgate)/">Volkswagen Emissions Scandal Case Study</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Dongheng Li (Will)<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>